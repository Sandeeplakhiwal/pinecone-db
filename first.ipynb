{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vectors found...\n",
      "Generated response:  model='phi' created_at='2025-03-08T19:51:28.752557377Z' done=True done_reason='stop' total_duration=7141287209 load_duration=16804314 prompt_eval_count=1131 prompt_eval_duration=5913000000 eval_count=33 eval_duration=1179000000 message=Message(role='assistant', content=\" I'm sorry, but due to character limitations, I cannot provide a text that is 3000 words long. Is there anything else I can assist you with?\\n\", images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import ollama\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text = extract_text(\"./pdfs/hess201.pdf\")\n",
    "print(\"text extracted...\")\n",
    "\n",
    "def create_text_chunks(text, chunk_size=500, overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,  chunk_overlap=overlap,  separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(\"chunks created...\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "def pc_db(index_name: str):\n",
    "    if not pc.has_index(index_name):\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            vector_type=\"dense\",\n",
    "            dimension=384,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ),\n",
    "            deletion_protection=\"disabled\",\n",
    "            tags={\n",
    "                \"environment\": \"development\"\n",
    "            }\n",
    "        )\n",
    "        print(\"PC created.\")\n",
    "\n",
    "    # Connect to the index\n",
    "    index = pc.Index(index_name)\n",
    "    return index\n",
    "\n",
    "\n",
    "dense_index = pc_db(\"textbook\")\n",
    "\n",
    "def upsert_to_db(chunks):\n",
    "    vectors = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = embed_model.embed_query(chunk)\n",
    "\n",
    "        vectors.append({\n",
    "            \"id\": f\"text-{i}\",\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"text\": chunk\n",
    "            }\n",
    "        })\n",
    "\n",
    "    print(f\"{len(vectors)} vectors prepared. Upserting...\")\n",
    "\n",
    "    batch_size = 96\n",
    "\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        dense_index.upsert(vectors=vectors[i:i+batch_size])\n",
    "        print(f\"Upserted batch {i // batch_size + 1}\")\n",
    "\n",
    "    print(\"All batches upserted successfully.\")\n",
    "\n",
    "        \n",
    "# upsert_to_db(create_text_chunks(text))\n",
    "\n",
    "def do_query(query):\n",
    "    query_embedding  = embed_model.embed_query(query)\n",
    "\n",
    "    results = dense_index.query(vector=query_embedding, top_k=10, include_metadata=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# print(\"Response: \",do_query(\"What is the name of this chapter?\"))\n",
    "\n",
    "def generate_response(query):\n",
    "    results = do_query(query)\n",
    "\n",
    "    print(\"Query vectors found...\")\n",
    "\n",
    "    if not results.get(\"matches\"):\n",
    "        print(\"No matches found.\")\n",
    "        return \"No relevant data found\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([r[\"metadata\"][\"text\"] for r in results[\"matches\"]])\n",
    "\n",
    "    # print(\"Context done...\", context)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    response = ollama.chat(model=\"llama2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"Generated response: \", generate_response(\"What is the name of this chapter?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
